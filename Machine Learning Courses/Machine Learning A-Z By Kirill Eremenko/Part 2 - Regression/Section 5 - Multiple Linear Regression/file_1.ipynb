{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37164bit10virtualenvcae4b91a0a4b4b17865e448f6050f74c",
   "display_name": "Python 3.7.1 64-bit ('10': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = pd.read_csv('50_Startups.csv')\n",
    "inputs = raw_csv_data.iloc[:, :-1].values\n",
    "targets = raw_csv_data.iloc[:, 4].values\n",
    "list(raw_csv_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(missing_values= 0, strategy='mean')\n",
    "\n",
    "imputer.fit(inputs[:, 0:3])\n",
    "inputs[:, 0:3] = imputer.transform(inputs[:, 0:3])\n",
    "inputs = pd.DataFrame(columns=list(raw_csv_data.columns[:-1]), data = inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R&amp;D Spend</th>\n      <th>Administration</th>\n      <th>Marketing Spend</th>\n      <th>State_1</th>\n      <th>State_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165349</td>\n      <td>136898</td>\n      <td>471784</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>162598</td>\n      <td>151378</td>\n      <td>443899</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>153442</td>\n      <td>101146</td>\n      <td>407935</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>144372</td>\n      <td>118672</td>\n      <td>383200</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>142107</td>\n      <td>91391.8</td>\n      <td>366168</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>131877</td>\n      <td>99814.7</td>\n      <td>362861</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>134615</td>\n      <td>147199</td>\n      <td>127717</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>130298</td>\n      <td>145530</td>\n      <td>323877</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>120543</td>\n      <td>148719</td>\n      <td>311613</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>123335</td>\n      <td>108679</td>\n      <td>304982</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>101913</td>\n      <td>110594</td>\n      <td>229161</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>100672</td>\n      <td>91790.6</td>\n      <td>249745</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>93863.8</td>\n      <td>127320</td>\n      <td>249839</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>91992.4</td>\n      <td>135495</td>\n      <td>252665</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>119943</td>\n      <td>156547</td>\n      <td>256513</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>114524</td>\n      <td>122617</td>\n      <td>261776</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>78013.1</td>\n      <td>121598</td>\n      <td>264346</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>94657.2</td>\n      <td>145078</td>\n      <td>282574</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>91749.2</td>\n      <td>114176</td>\n      <td>294920</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>86419.7</td>\n      <td>153514</td>\n      <td>224495</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>76253.9</td>\n      <td>113867</td>\n      <td>298664</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>78389.5</td>\n      <td>153773</td>\n      <td>299737</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>73994.6</td>\n      <td>122783</td>\n      <td>303319</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>67532.5</td>\n      <td>105751</td>\n      <td>304769</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>77044</td>\n      <td>99281.3</td>\n      <td>140575</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>64664.7</td>\n      <td>139553</td>\n      <td>137963</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>75328.9</td>\n      <td>144136</td>\n      <td>134050</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>72107.6</td>\n      <td>127865</td>\n      <td>353184</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>66051.5</td>\n      <td>182646</td>\n      <td>118148</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>65605.5</td>\n      <td>153032</td>\n      <td>107138</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>61994.5</td>\n      <td>115641</td>\n      <td>91131.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>61136.4</td>\n      <td>152702</td>\n      <td>88218.2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>63408.9</td>\n      <td>129220</td>\n      <td>46085.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>55493.9</td>\n      <td>103057</td>\n      <td>214635</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>46426.1</td>\n      <td>157694</td>\n      <td>210798</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>46014</td>\n      <td>85047.4</td>\n      <td>205518</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>28663.8</td>\n      <td>127056</td>\n      <td>201127</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>44069.9</td>\n      <td>51283.1</td>\n      <td>197029</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>20229.6</td>\n      <td>65947.9</td>\n      <td>185265</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>38558.5</td>\n      <td>82982.1</td>\n      <td>174999</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>28754.3</td>\n      <td>118546</td>\n      <td>172796</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>27892.9</td>\n      <td>84710.8</td>\n      <td>164471</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>23640.9</td>\n      <td>96189.6</td>\n      <td>148001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>15505.7</td>\n      <td>127382</td>\n      <td>35534.2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>22177.7</td>\n      <td>154806</td>\n      <td>28334.7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>1000.23</td>\n      <td>124153</td>\n      <td>1903.93</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>1315.46</td>\n      <td>115816</td>\n      <td>297114</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>76793.3</td>\n      <td>135427</td>\n      <td>224495</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>542.05</td>\n      <td>51743.2</td>\n      <td>224495</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>76793.3</td>\n      <td>116984</td>\n      <td>45173.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   R&D Spend Administration Marketing Spend  State_1  State_2\n0     165349         136898          471784      0.0      1.0\n1     162598         151378          443899      0.0      0.0\n2     153442         101146          407935      1.0      0.0\n3     144372         118672          383200      0.0      1.0\n4     142107        91391.8          366168      1.0      0.0\n5     131877        99814.7          362861      0.0      1.0\n6     134615         147199          127717      0.0      0.0\n7     130298         145530          323877      1.0      0.0\n8     120543         148719          311613      0.0      1.0\n9     123335         108679          304982      0.0      0.0\n10    101913         110594          229161      1.0      0.0\n11    100672        91790.6          249745      0.0      0.0\n12   93863.8         127320          249839      1.0      0.0\n13   91992.4         135495          252665      0.0      0.0\n14    119943         156547          256513      1.0      0.0\n15    114524         122617          261776      0.0      1.0\n16   78013.1         121598          264346      0.0      0.0\n17   94657.2         145078          282574      0.0      1.0\n18   91749.2         114176          294920      1.0      0.0\n19   86419.7         153514          224495      0.0      1.0\n20   76253.9         113867          298664      0.0      0.0\n21   78389.5         153773          299737      0.0      1.0\n22   73994.6         122783          303319      1.0      0.0\n23   67532.5         105751          304769      1.0      0.0\n24     77044        99281.3          140575      0.0      1.0\n25   64664.7         139553          137963      0.0      0.0\n26   75328.9         144136          134050      1.0      0.0\n27   72107.6         127865          353184      0.0      1.0\n28   66051.5         182646          118148      1.0      0.0\n29   65605.5         153032          107138      0.0      1.0\n30   61994.5         115641         91131.2      1.0      0.0\n31   61136.4         152702         88218.2      0.0      1.0\n32   63408.9         129220         46085.2      0.0      0.0\n33   55493.9         103057          214635      1.0      0.0\n34   46426.1         157694          210798      0.0      0.0\n35     46014        85047.4          205518      0.0      1.0\n36   28663.8         127056          201127      1.0      0.0\n37   44069.9        51283.1          197029      0.0      0.0\n38   20229.6        65947.9          185265      0.0      1.0\n39   38558.5        82982.1          174999      0.0      0.0\n40   28754.3         118546          172796      0.0      0.0\n41   27892.9        84710.8          164471      1.0      0.0\n42   23640.9        96189.6          148001      0.0      0.0\n43   15505.7         127382         35534.2      0.0      1.0\n44   22177.7         154806         28334.7      0.0      0.0\n45   1000.23         124153         1903.93      0.0      1.0\n46   1315.46         115816          297114      1.0      0.0\n47   76793.3         135427          224495      0.0      0.0\n48    542.05        51743.2          224495      0.0      1.0\n49   76793.3         116984         45173.1      0.0      0.0"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "le_state = LabelEncoder()\n",
    "inputs.State = le_state.fit_transform(inputs.State)\n",
    "\n",
    "ohe_state = OneHotEncoder()\n",
    "inputs_state_encoded = ohe_state.fit_transform(inputs.State.values.reshape(-1,1)).toarray()\n",
    "df_ohe = pd.DataFrame(inputs_state_encoded, columns=[\"State_\"+str(int(i)) for i in range(inputs_state_encoded.shape[1])])\n",
    "inputs_ohe_mod = pd.concat([inputs,df_ohe], axis=1)\n",
    "inputs_ohe_mod = inputs_ohe_mod.drop(['State','State_0'],axis=1)\n",
    "inputs_ohe_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_preprocessed = inputs_ohe_mod.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R&amp;D Spend</th>\n      <th>Administration</th>\n      <th>Marketing Spend</th>\n      <th>State_1</th>\n      <th>State_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>33</th>\n      <td>55493.9</td>\n      <td>103057</td>\n      <td>214635</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>46014</td>\n      <td>85047.4</td>\n      <td>205518</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>75328.9</td>\n      <td>144136</td>\n      <td>134050</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>46426.1</td>\n      <td>157694</td>\n      <td>210798</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>91749.2</td>\n      <td>114176</td>\n      <td>294920</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>130298</td>\n      <td>145530</td>\n      <td>323877</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>119943</td>\n      <td>156547</td>\n      <td>256513</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>1000.23</td>\n      <td>124153</td>\n      <td>1903.93</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>542.05</td>\n      <td>51743.2</td>\n      <td>224495</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>65605.5</td>\n      <td>153032</td>\n      <td>107138</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>114524</td>\n      <td>122617</td>\n      <td>261776</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>61994.5</td>\n      <td>115641</td>\n      <td>91131.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>63408.9</td>\n      <td>129220</td>\n      <td>46085.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>78013.1</td>\n      <td>121598</td>\n      <td>264346</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>23640.9</td>\n      <td>96189.6</td>\n      <td>148001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>76253.9</td>\n      <td>113867</td>\n      <td>298664</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>15505.7</td>\n      <td>127382</td>\n      <td>35534.2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>120543</td>\n      <td>148719</td>\n      <td>311613</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>91992.4</td>\n      <td>135495</td>\n      <td>252665</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>64664.7</td>\n      <td>139553</td>\n      <td>137963</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>131877</td>\n      <td>99814.7</td>\n      <td>362861</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>94657.2</td>\n      <td>145078</td>\n      <td>282574</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>28754.3</td>\n      <td>118546</td>\n      <td>172796</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>76793.3</td>\n      <td>116984</td>\n      <td>45173.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>162598</td>\n      <td>151378</td>\n      <td>443899</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>93863.8</td>\n      <td>127320</td>\n      <td>249839</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>44069.9</td>\n      <td>51283.1</td>\n      <td>197029</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>77044</td>\n      <td>99281.3</td>\n      <td>140575</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>134615</td>\n      <td>147199</td>\n      <td>127717</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>67532.5</td>\n      <td>105751</td>\n      <td>304769</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>28663.8</td>\n      <td>127056</td>\n      <td>201127</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>78389.5</td>\n      <td>153773</td>\n      <td>299737</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>86419.7</td>\n      <td>153514</td>\n      <td>224495</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>123335</td>\n      <td>108679</td>\n      <td>304982</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>38558.5</td>\n      <td>82982.1</td>\n      <td>174999</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>1315.46</td>\n      <td>115816</td>\n      <td>297114</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>144372</td>\n      <td>118672</td>\n      <td>383200</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>165349</td>\n      <td>136898</td>\n      <td>471784</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>76793.3</td>\n      <td>135427</td>\n      <td>224495</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>22177.7</td>\n      <td>154806</td>\n      <td>28334.7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   R&D Spend Administration Marketing Spend  State_1  State_2\n33   55493.9         103057          214635      1.0      0.0\n35     46014        85047.4          205518      0.0      1.0\n26   75328.9         144136          134050      1.0      0.0\n34   46426.1         157694          210798      0.0      0.0\n18   91749.2         114176          294920      1.0      0.0\n7     130298         145530          323877      1.0      0.0\n14    119943         156547          256513      1.0      0.0\n45   1000.23         124153         1903.93      0.0      1.0\n48    542.05        51743.2          224495      0.0      1.0\n29   65605.5         153032          107138      0.0      1.0\n15    114524         122617          261776      0.0      1.0\n30   61994.5         115641         91131.2      1.0      0.0\n32   63408.9         129220         46085.2      0.0      0.0\n16   78013.1         121598          264346      0.0      0.0\n42   23640.9        96189.6          148001      0.0      0.0\n20   76253.9         113867          298664      0.0      0.0\n43   15505.7         127382         35534.2      0.0      1.0\n8     120543         148719          311613      0.0      1.0\n13   91992.4         135495          252665      0.0      0.0\n25   64664.7         139553          137963      0.0      0.0\n5     131877        99814.7          362861      0.0      1.0\n17   94657.2         145078          282574      0.0      1.0\n40   28754.3         118546          172796      0.0      0.0\n49   76793.3         116984         45173.1      0.0      0.0\n1     162598         151378          443899      0.0      0.0\n12   93863.8         127320          249839      1.0      0.0\n37   44069.9        51283.1          197029      0.0      0.0\n24     77044        99281.3          140575      0.0      1.0\n6     134615         147199          127717      0.0      0.0\n23   67532.5         105751          304769      1.0      0.0\n36   28663.8         127056          201127      1.0      0.0\n21   78389.5         153773          299737      0.0      1.0\n19   86419.7         153514          224495      0.0      1.0\n9     123335         108679          304982      0.0      0.0\n39   38558.5        82982.1          174999      0.0      0.0\n46   1315.46         115816          297114      1.0      0.0\n3     144372         118672          383200      0.0      1.0\n0     165349         136898          471784      0.0      1.0\n47   76793.3         135427          224495      0.0      0.0\n44   22177.7         154806         28334.7      0.0      0.0"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs_preprocessed,targets, test_size=0.2, random_state=0)\n",
    "inputs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(inputs_train,targets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([102388.94113038, 121465.72713521, 127340.57708621,  71709.47538914,\n       174211.08480002, 121771.6506149 ,  68393.54360669,  95588.53133487,\n       116596.34676989, 162514.07218554])"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = reg.predict(inputs_test)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the optimal model using Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.00000000e+00, 1.65349200e+05, 1.36897800e+05, 4.71784100e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 1.62597700e+05, 1.51377590e+05, 4.43898530e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.53441510e+05, 1.01145550e+05, 4.07934540e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.44372410e+05, 1.18671850e+05, 3.83199620e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 1.42107340e+05, 9.13917700e+04, 3.66168420e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.31876900e+05, 9.98147100e+04, 3.62861360e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 1.34615460e+05, 1.47198870e+05, 1.27716820e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.30298130e+05, 1.45530060e+05, 3.23876680e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.20542520e+05, 1.48718950e+05, 3.11613290e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 1.23334880e+05, 1.08679170e+05, 3.04981620e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.01913080e+05, 1.10594110e+05, 2.29160950e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.00671960e+05, 9.17906100e+04, 2.49744550e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 9.38637500e+04, 1.27320380e+05, 2.49839440e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 9.19923900e+04, 1.35495070e+05, 2.52664930e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.19943240e+05, 1.56547420e+05, 2.56512920e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.14523610e+05, 1.22616840e+05, 2.61776230e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 7.80131100e+04, 1.21597550e+05, 2.64346060e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 9.46571600e+04, 1.45077580e+05, 2.82574310e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 9.17491600e+04, 1.14175790e+05, 2.94919570e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 8.64197000e+04, 1.53514110e+05, 2.24494785e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 7.62538600e+04, 1.13867300e+05, 2.98664470e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 7.83894700e+04, 1.53773430e+05, 2.99737290e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 7.39945600e+04, 1.22782750e+05, 3.03319260e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 6.75325300e+04, 1.05751030e+05, 3.04768730e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 7.70440100e+04, 9.92813400e+04, 1.40574810e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 6.46647100e+04, 1.39553160e+05, 1.37962620e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 7.53288700e+04, 1.44135980e+05, 1.34050070e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 7.21076000e+04, 1.27864550e+05, 3.53183810e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 6.60515200e+04, 1.82645560e+05, 1.18148200e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 6.56054800e+04, 1.53032060e+05, 1.07138380e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 6.19944800e+04, 1.15641280e+05, 9.11312400e+04,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 6.11363800e+04, 1.52701920e+05, 8.82182300e+04,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 6.34088600e+04, 1.29219610e+05, 4.60852500e+04,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 5.54939500e+04, 1.03057490e+05, 2.14634810e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 4.64260700e+04, 1.57693920e+05, 2.10797670e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 4.60140200e+04, 8.50474400e+04, 2.05517640e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 2.86637600e+04, 1.27056210e+05, 2.01126820e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 4.40699500e+04, 5.12831400e+04, 1.97029420e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 2.02295900e+04, 6.59479300e+04, 1.85265100e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 3.85585100e+04, 8.29820900e+04, 1.74999300e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 2.87543300e+04, 1.18546050e+05, 1.72795670e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 2.78929200e+04, 8.47107700e+04, 1.64470710e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 2.36409300e+04, 9.61896300e+04, 1.48001110e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.55057300e+04, 1.27382300e+05, 3.55341700e+04,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 2.21777400e+04, 1.54806140e+05, 2.83347200e+04,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 1.00023000e+03, 1.24153040e+05, 1.90393000e+03,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 1.31546000e+03, 1.15816210e+05, 2.97114460e+05,\n        1.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 7.67933496e+04, 1.35426920e+05, 2.24494785e+05,\n        0.00000000e+00, 0.00000000e+00],\n       [1.00000000e+00, 5.42050000e+02, 5.17431500e+04, 2.24494785e+05,\n        0.00000000e+00, 1.00000000e+00],\n       [1.00000000e+00, 7.67933496e+04, 1.16983800e+05, 4.51730600e+04,\n        0.00000000e+00, 0.00000000e+00]])"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs_backward_elimination = np.append(values=inputs_backward_elimination, arr=np.ones((50,1)).astype(int), axis=1)\n",
    "inputs_opt = sm.add_constant(inputs_preprocessed)\n",
    "inputs_opt = np.asarray(inputs_opt, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.802</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.780</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   35.69</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 04 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>2.02e-14</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:02:26</td>     <th>  Log-Likelihood:    </th> <td> -560.14</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1132.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1144.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 3.813e+04</td> <td> 1.43e+04</td> <td>    2.665</td> <td> 0.011</td> <td> 9288.988</td> <td>  6.7e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.7076</td> <td>    0.093</td> <td>    7.591</td> <td> 0.000</td> <td>    0.520</td> <td>    0.895</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.0103</td> <td>    0.107</td> <td>    0.096</td> <td> 0.924</td> <td>   -0.206</td> <td>    0.226</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.0633</td> <td>    0.036</td> <td>    1.750</td> <td> 0.087</td> <td>   -0.010</td> <td>    0.136</td>\n</tr>\n<tr>\n  <th>x4</th>    <td> 6619.5300</td> <td> 6757.696</td> <td>    0.980</td> <td> 0.333</td> <td>-6999.712</td> <td> 2.02e+04</td>\n</tr>\n<tr>\n  <th>x5</th>    <td> 5793.7942</td> <td> 6594.305</td> <td>    0.879</td> <td> 0.384</td> <td>-7496.154</td> <td> 1.91e+04</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>54.528</td> <th>  Durbin-Watson:     </th> <td>   0.875</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 278.512</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-2.917</td> <th>  Prob(JB):          </th> <td>3.33e-61</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td>12.983</td> <th>  Cond. No.          </th> <td>1.53e+06</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.53e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems.",
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.802\nModel:                            OLS   Adj. R-squared:                  0.780\nMethod:                 Least Squares   F-statistic:                     35.69\nDate:                Wed, 04 Mar 2020   Prob (F-statistic):           2.02e-14\nTime:                        01:02:26   Log-Likelihood:                -560.14\nNo. Observations:                  50   AIC:                             1132.\nDf Residuals:                      44   BIC:                             1144.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       3.813e+04   1.43e+04      2.665      0.011    9288.988     6.7e+04\nx1             0.7076      0.093      7.591      0.000       0.520       0.895\nx2             0.0103      0.107      0.096      0.924      -0.206       0.226\nx3             0.0633      0.036      1.750      0.087      -0.010       0.136\nx4          6619.5300   6757.696      0.980      0.333   -6999.712    2.02e+04\nx5          5793.7942   6594.305      0.879      0.384   -7496.154    1.91e+04\n==============================================================================\nOmnibus:                       54.528   Durbin-Watson:                   0.875\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              278.512\nSkew:                          -2.917   Prob(JB):                     3.33e-61\nKurtosis:                      12.983   Cond. No.                     1.53e+06\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.53e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\""
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_OLS = sm.OLS(targets,inputs_opt).fit()\n",
    "reg_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}